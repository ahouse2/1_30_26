x-gpu-capable: &gpu-capable
  deploy:
    resources:
      reservations:
        devices:
          - capabilities:
              - gpu
            count: ${GPU_DEVICE_COUNT:-0}
            driver: ${GPU_DRIVER:-nvidia}

services:
  api:
    build:
      context: ./backend
    container_name: cocounsel_api
    profiles:
      - dev
      - prod
    ports:
      - "8000:8000"
    restart: unless-stopped
    depends_on:
      neo4j:
        condition: service_started
      qdrant:
        condition: service_started
    environment:
      - PYTHONUNBUFFERED=1
      - MODEL_PROVIDERS_PRIMARY=${MODEL_PROVIDERS_PRIMARY:-gemini}
      - MODEL_PROVIDERS_SECONDARY=${MODEL_PROVIDERS_SECONDARY:-openai}
      - DEFAULT_CHAT_MODEL=${DEFAULT_CHAT_MODEL:-gemini-2.5-flash}
      - DEFAULT_EMBEDDING_MODEL=${DEFAULT_EMBEDDING_MODEL:-text-embedding-004}
      - DEFAULT_VISION_MODEL=${DEFAULT_VISION_MODEL:-gemini-2.5-flash}
      - NEO4J_URI=${NEO4J_URI:-neo4j://neo4j:7687}
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-neo4j}
      - NEO4J_PASSWORD_FILE=/run/secrets/neo4j_password
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - VECTOR_BACKEND=${VECTOR_BACKEND:-qdrant}
      - VECTOR_DIR=/data/vector
      - INGESTION_CHROMA_DIR=/var/cocounsel/chroma
      - INGESTION_LLAMA_CACHE_DIR=/var/cocounsel/llama_cache
      - VOICE_SESSIONS_DIR=/data/voice/sessions
      - VOICE_CACHE_DIR=/models
      - DOCUMENT_STORAGE_PATH=/var/cocounsel/documents
      - WORKFLOW_STORAGE_PATH=/var/cocounsel/workflows
      - JOB_STORE_DIR=/var/cocounsel/jobs
      - INGESTION_WORKSPACE_DIR=/var/cocounsel/workspaces
      - AGENT_THREADS_DIR=/var/cocounsel/agent_threads
      - FORENSICS_DIR=/var/cocounsel/forensics
      - TIMELINE_STORAGE_PATH=/var/cocounsel/timelines
      - GRAPH_SNAPSHOT_PATH=/var/cocounsel/graphs
      - TELEMETRY_BUFFER_PATH=/var/cocounsel/telemetry
      - BILLING_USAGE_PATH=/data/billing/usage.json
      - BILLING_DEFAULT_PLAN=${BILLING_DEFAULT_PLAN:-community}
      - TELEMETRY_ENABLED=${TELEMETRY_ENABLED:-false}
      - TELEMETRY_OTLP_ENDPOINT=${TELEMETRY_OTLP_ENDPOINT:-http://otel-collector:4317}
      - TELEMETRY_OTLP_INSECURE=${TELEMETRY_OTLP_INSECURE:-true}
      - TELEMETRY_ENVIRONMENT=${TELEMETRY_ENVIRONMENT:-community}
      - GRAPH_REFINEMENT_ENABLED=${GRAPH_REFINEMENT_ENABLED:-true}
      - GRAPH_REFINEMENT_INTERVAL_SECONDS=${GRAPH_REFINEMENT_INTERVAL_SECONDS:-900}
      - GRAPH_REFINEMENT_IDLE_LIMIT=${GRAPH_REFINEMENT_IDLE_LIMIT:-3}
      - GRAPH_REFINEMENT_MIN_NEW_EDGES=${GRAPH_REFINEMENT_MIN_NEW_EDGES:-0}
      - STT_SERVICE_URL=${STT_SERVICE_URL:-http://stt:9000}
      - TTS_SERVICE_URL=${TTS_SERVICE_URL:-http://tts:5002}
      - HUGGINGFACE_HUB_CACHE=/var/cocounsel/models/huggingface
      - WHISPER_MODEL_PATH=/var/cocounsel/models/whisper
      - TTS_MODEL_PATH=/var/cocounsel/models/tts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 15s
      timeout: 5s
      retries: 6
    volumes:
      - api_data:/data
      - voice_models:/models
      - ./var/storage/documents:/var/cocounsel/documents
      - ./var/storage/workflows:/var/cocounsel/workflows
      - ./var/storage/jobs:/var/cocounsel/jobs
      - ./var/storage/workspaces:/var/cocounsel/workspaces
      - ./var/storage/agent_threads:/var/cocounsel/agent_threads
      - ./var/storage/forensics:/var/cocounsel/forensics
      - ./var/storage/timelines:/var/cocounsel/timelines
      - ./var/storage/llama_cache:/var/cocounsel/llama_cache
      - ./var/storage/chroma:/var/cocounsel/chroma
      - ./var/storage/graphs:/var/cocounsel/graphs
      - ./var/storage/telemetry:/var/cocounsel/telemetry
      - ./var/models/huggingface:/var/cocounsel/models/huggingface
      - ./var/models/whisper:/var/cocounsel/models/whisper
      - ./var/models/tts:/var/cocounsel/models/tts
    secrets:
      - neo4j_password
    networks:
      - backend

  frontend:
    build:
      context: ./frontend
    container_name: cocounsel_frontend
    profiles:
      - dev
    ports:
      - "5173:5173"
    restart: unless-stopped
    depends_on:
      api:
        condition: service_healthy
    environment:
      - CHOKIDAR_USEPOLLING=1
      - VITE_API_BASE_URL=http://api:8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5173/"]
      interval: 15s
      timeout: 5s
      retries: 6
    networks:
      - backend

  frontend-prod:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    container_name: cocounsel_frontend_prod
    profiles:
      - prod
    ports:
      - "80:80"
    restart: unless-stopped
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost/"]
      interval: 15s
      timeout: 5s
      retries: 6
    networks:
      - backend

  neo4j:
    image: neo4j:5.20
    container_name: cocounsel_neo4j
    profiles:
      - dev
      - prod
    ports:
      - "7687:7687"
      - "7474:7474"
    volumes:
      - neo4j_data:/data
      - ./infra/migrations/neo4j:/var/lib/neo4j/migrations:ro
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-neo4j}
      - NEO4J_dbms_memory_heap_initial__size=1G
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_security_auth__enabled=true
      - NEO4J_PASSWORD_FILE=/run/secrets/neo4j_password
    secrets:
      - neo4j_password
    restart: unless-stopped
    networks:
      - backend

  qdrant:
    image: qdrant/qdrant:latest
    container_name: cocounsel_qdrant
    profiles:
      - dev
      - prod
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    networks:
      - backend

  stt:
    <<: *gpu-capable
    image: linuxserver/faster-whisper:latest
    profiles:
      - voice
    environment:
      - ASR_MODEL=${STT_MODEL_NAME:-openai/whisper-small}
      - ASR_ENGINE=faster_whisper
      - ASR_BEAM_SIZE=5
      - ASR_DEVICE=${STT_DEVICE:-cpu}
      - HUGGINGFACE_HUB_CACHE=/models/huggingface
      - ASR_OUTPUT_LANGUAGE=${STT_OUTPUT_LANGUAGE:-en}
    ports:
      - "9000:10300"
    volumes:
      - ./var/models/huggingface:/models/huggingface
      - ./var/models/whisper:/models/whisper
    networks:
      - backend

  tts:
    <<: *gpu-capable
    image: rhasspy/larynx:latest
    profiles:
      - voice
    environment:
      - LARYNX_VOICE=${TTS_VOICE:-en-us-blizzard_lessac}
      - LARYNX_OUTPUT_DIR=/output
      - HUGGINGFACE_HUB_CACHE=/models/huggingface
    ports:
      - "5002:5002"
    volumes:
      - ./var/models/huggingface:/models/huggingface
      - ./var/models/tts:/models/tts
      - ./var/audio:/output
    networks:
      - backend

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.98.0
    profiles:
      - prod
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./infra/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"
      - "9464:9464"
    networks:
      - backend

  grafana:
    image: grafana/grafana-oss:11.1.4
    profiles:
      - prod
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s/grafana/
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infra/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./infra/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - otel-collector
    networks:
      - backend

  storage-backup:
    image: ghcr.io/offen/docker-volume-backup:latest
    profiles:
      - prod
    environment:
      - BACKUP_CRON=${BACKUP_CRON_SCHEDULE:-0 3 * * *}
      - BACKUP_FILENAME=full-stack
      - BACKUP_PATH=/var/backups
      - BACKUP_PRUNING_PREFIX=full-stack
      - BACKUP_PRUNING_KEEP_DAYS=${BACKUP_RETENTION_DAYS:-7}
      - BACKUP_LATEST_SYMLINK=true
    volumes:
      - ./var/backups:/var/backups
      - ./var/storage/documents:/backup/documents:ro
      - ./var/storage/graphs:/backup/graphs:ro
      - ./var/storage/telemetry:/backup/telemetry:ro
    networks:
      - backend

networks:
  backend:
    driver: bridge

secrets:
  neo4j_password:
    file: docker/secrets/neo4j_password.txt

volumes:
  neo4j_data:
  qdrant_data:
  api_data:
  voice_models:
  grafana_data:
